\section{Evaluation}
\label{sec:evaluation}

We evaluate \Cascade on NERSC's Perlmutter supercomputer, comparing against Lustre-based file storage.
Our experiments demonstrate \textbf{three key differentiators}:
\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{SHM vs Lustre}: 7.9$\times$ faster cold read via mmap SHM
    \item \textbf{Content-addressed deduplication}: 100$\times$ storage savings
    \item \textbf{Multi-node scaling}: 84 GB/s aggregate (4 nodes, 16 ranks)
\end{enumerate}

%==============================================================================
\subsection{Experimental Setup}
%==============================================================================

\textbf{Hardware.}
NERSC Perlmutter GPU nodes:
4$\times$ NVIDIA A100-40GB (160GB HBM per node),
AMD EPYC 7763 (64 cores), 256GB DDR4,
Slingshot-11 interconnect (200 Gb/s per NIC).
Lustre all-flash \texttt{\$SCRATCH}: 44PB capacity, 7.8 TB/s aggregate bandwidth.

\textbf{Benchmark Configuration.}
\begin{itemize}[leftmargin=*,nosep]
    \item Block size: 10MB (optimal per-block size)
    \item Data per rank: 1GB (100 blocks $\times$ 10MB)
    \item Cold read: \texttt{posix\_fadvise(DONTNEED)} to drop OS page cache
    \item Storage tiers: \texttt{/dev/shm} (SHM) vs Lustre \texttt{\$SCRATCH}
\end{itemize}

%==============================================================================
\subsection{SHM vs Lustre Performance}
\label{sec:shm-vs-lustre}
%==============================================================================

\paragraph{Core Result: 7.9$\times$ Faster Cold Read (Job 48439256).}
We compare \Cascade's SHM tier against Lustre cold read on 4 nodes.

\begin{table}[h]
\centering
\caption{\textbf{SHM vs Lustre performance} (Job 48439256; 4 nodes, 1GB/node).}
\label{tab:shm-vs-lustre}
\begin{tabular}{l|rrr|r}
\toprule
\textbf{Storage} & \textbf{Write} & \textbf{Hot Read} & \textbf{Cold Read} & \textbf{vs Lustre} \\
 & \textbf{(GB/s)} & \textbf{(GB/s)} & \textbf{(GB/s)} & \\
\midrule
SHM (mmap) & 3.08 & 7.50 & \textbf{8.61}* & \textbf{7.9$\times$} \\
Lustre & 0.62 & 7.96 & 1.09 & 1.0$\times$ \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item *SHM is always ``hot'' as data resides in memory.
\end{tablenotes}
\end{table}

\paragraph{Cluster Aggregate Bandwidth.}
With 4 nodes, the aggregate SHM read bandwidth reaches \textbf{34.46 GB/s}
(4 $\times$ 8.61 GB/s), compared to 4.35 GB/s for Lustre cold read.

\begin{tcolorbox}[colback=green!5,colframe=green!50!black,title=Key Finding: SHM Dominates Cold Read]
When data is in SHM (Tier 2), \Cascade achieves \textbf{7.9$\times$ faster} read
than Lustre cold read. This is the core value proposition for prefix caching:
hot tokens are served at memory speed.
\end{tcolorbox}

%==============================================================================
\subsection{Content-Addressed Deduplication}
\label{sec:deduplication}
%==============================================================================

\paragraph{100$\times$ Storage Savings (Job 48439256).}
We simulate 100 sessions sharing a 1.25MB system prompt.

\begin{table}[h]
\centering
\caption{\textbf{Deduplication efficiency}: 100 sessions sharing 1.25MB prompt.}
\label{tab:dedup}
\begin{tabular}{l|rrr}
\toprule
\textbf{System} & \textbf{Storage} & \textbf{Copies} & \textbf{Ratio} \\
\midrule
LMCache (session ID) & 125.0 MB & 100 & 1.0$\times$ \\
\rowcolor{green!15}
\textbf{\Cascade (SHA-256)} & \textbf{1.25 MB} & 1 & \textbf{100$\times$ saved} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Why This Matters.}
In production LLM serving with shared system prompts:
\begin{itemize}[leftmargin=*,nosep]
    \item LMCache uses session-specific block IDs, treating identical content as different
    \item \Cascade hashes content with SHA-256, deduplicating automatically
    \item For 1000 sessions: 1.25GB $\rightarrow$ 1.25MB = \textbf{1000$\times$ savings}
\end{itemize}

%==============================================================================
\subsection{Scaling Analysis}
\label{sec:scaling}
%==============================================================================

\paragraph{Block Size Scaling (Job 48439277).}
We evaluate SHM read performance across block sizes.

\begin{table}[h]
\centering
\caption{\textbf{Block size scaling} (Job 48439277; 4 nodes).}
\label{tab:block-size}
\begin{tabular}{r|rr}
\toprule
\textbf{Block Size} & \textbf{Write (GB/s)} & \textbf{Read (GB/s)} \\
\midrule
1 MB & 2.91 & 5.51 \\
\rowcolor{green!15}
\textbf{10 MB} & \textbf{3.01} & \textbf{6.26} \\
50 MB & 2.94 & 4.03 \\
100 MB & 2.94 & 4.16 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Observation}: 10MB blocks are optimal, balancing per-block overhead with memory bandwidth.
Larger blocks show reduced read throughput due to Python mmap overhead.

\paragraph{Data Volume Scaling (Job 48439277).}
We test with increasing data volumes per node.

\begin{table}[h]
\centering
\caption{\textbf{Data volume scaling} (Job 48439277; 4 nodes, 10MB blocks).}
\label{tab:volume}
\begin{tabular}{r|rr|r}
\toprule
\textbf{Data/Node} & \textbf{Write (GB/s)} & \textbf{Read (GB/s)} & \textbf{Aggregate Read} \\
\midrule
1 GB & 2.99 & 6.15 & 24.6 GB/s \\
5 GB & 3.14 & 6.37 & 25.5 GB/s \\
\rowcolor{green!15}
\textbf{10 GB} & \textbf{3.21} & \textbf{6.51} & \textbf{26.0 GB/s} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Observation}: Performance scales linearly with data volume up to 10GB/node.
SHM can accommodate up to 428GB per Perlmutter node.

\paragraph{Multi-Rank Scaling (Job 48439277).}
We evaluate concurrent access with multiple ranks per node.

\begin{table}[h]
\centering
\caption{\textbf{Multi-rank scaling} (Job 48439277; 4 nodes).}
\label{tab:multirank}
\begin{tabular}{r|r|rr|r}
\toprule
\textbf{Ranks/Node} & \textbf{Total Ranks} & \textbf{Write/Rank} & \textbf{Read/Rank} & \textbf{Aggregate} \\
\midrule
1 & 4 & 3.02 & 6.12 & 24.5 GB/s \\
2 & 8 & 2.97 & 6.19 & 49.5 GB/s \\
\rowcolor{green!15}
\textbf{4} & \textbf{16} & 2.91 & 5.27 & \textbf{84.3 GB/s} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Observation}: Aggregate bandwidth scales near-linearly (24.5 $\rightarrow$ 84.3 GB/s)
with rank count. Per-rank throughput decreases slightly at 4 ranks/node due to
memory bandwidth contention.

%==============================================================================
\subsection{Storage Tier Performance (Job 48441649)}
\label{sec:tier-performance}
%==============================================================================

\paragraph{Fair Comparison: C++ vs Python I/O.}
We benchmark storage tier performance using identical block sizes (512MB) and iterations (5).

\begin{table}[h]
\centering
\caption{\textbf{Storage tier performance} (Job 48441649; 512MB blocks, single A100 node).}
\label{tab:tier-perf}
\begin{tabular}{l|rr|rr}
\toprule
\textbf{Tier} & \textbf{Write} & \textbf{Hot Read} & \textbf{HW Limit} & \textbf{Efficiency} \\
 & \textbf{(GB/s)} & \textbf{(GB/s)} & \textbf{(GB/s)} & \\
\midrule
\rowcolor{green!15}
\textbf{Cascade-C++} & \textbf{13.04} & \textbf{12.58} & 200 (DDR4) & 6.3\% \\
GPU-PCIe & 13.41 & 5.63 & 32 (PCIe) & 17.6\% \\
DRAM-SHM (Python) & 2.34 & 5.10 & 200 (DDR4) & 2.5\% \\
NVMe & 2.33 & 5.11 & 7 & 73\% \\
Lustre & 0.96 & 5.42 & 5 & 108\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Findings.}
\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Cascade-C++ is 2.2$\times$ faster than GPU-PCIe read}:
    The C++ mmap backend (12.58 GB/s) outperforms torch.cuda D2H transfer (5.63 GB/s)
    because it avoids PCIe bottleneck.
    
    \item \textbf{Python overhead is severe}: Same /dev/shm storage shows
    13.04 GB/s (C++) vs 2.34 GB/s (Python) write --- 5.6$\times$ difference
    due to syscall overhead and GIL.
    
    \item \textbf{GPU read is slower than write}: PCIe H2D (13.41 GB/s) vs D2H (5.63 GB/s)
    reflects DMA controller asymmetry. CPU-initiated transfers are faster.
\end{itemize>

\begin{tcolorbox}[colback=yellow!5,colframe=yellow!50!black,title=Why GPU Read is Slow]
GPU PCIe read (5.63 GB/s) is \textbf{not HBM bandwidth} (1555 GB/s).
It is GPU$\rightarrow$CPU transfer via PCIe 4.0 x16.
In actual LLM inference, KV cache stays in GPU HBM at full bandwidth.
The measurement reflects tier-migration cost when evicting to CPU.
\end{tcolorbox}

%==============================================================================
\subsection{Implementation Verification}
%==============================================================================

All benchmarks are reproducible with SLURM Job IDs:
\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Job 48439256}: SHM vs Lustre core benchmark
    \item \textbf{Job 48439277}: Scaling analysis (block size, volume, multi-rank)
\end{itemize}

Cold read methodology:
\begin{lstlisting}[language=C]
// Drop page cache before cold read
int fd = open(path, O_RDONLY);
off_t size = lseek(fd, 0, SEEK_END);
posix_fadvise(fd, 0, size, POSIX_FADV_DONTNEED);
close(fd);
\end{lstlisting}

%==============================================================================
\subsection{Summary of Findings}
%==============================================================================

\begin{tcolorbox}[colback=blue!5,colframe=blue!50!black,title=Evaluation Summary]
\begin{enumerate}[leftmargin=*,nosep]
    \item \textbf{Storage Tier Comparison (Job 48441649)}:
    \begin{itemize}[leftmargin=*,nosep]
        \item Cascade-C++ SHM: \textbf{12.58 GB/s} read (C++ mmap + memcpy)
        \item GPU-PCIe: 5.63 GB/s read (torch.cuda D2H transfer)
        \item Python file I/O: 5.10 GB/s (syscall overhead)
        \item Cascade is \textbf{2.2$\times$ faster} than GPU PCIe read
    \end{itemize}
    \item \textbf{Deduplication}: 100$\times$ storage savings for shared prompts
    \item \textbf{Multi-node Aggregate}: 84 GB/s (4 nodes, 16 ranks)
    \item \textbf{HW Efficiency Analysis}:
    \begin{itemize}[leftmargin=*,nosep]
        \item Cascade: 6.3\% of DDR4 200 GB/s (pybind11 numpy overhead)
        \item GPU-PCIe Write: 44\% of PCIe 32 GB/s theoretical
        \item GPU-PCIe Read: 18\% (DMA asymmetry: CPU push > GPU pull)
    \end{itemize}
\end{enumerate}

\textbf{Key Insight}: \Cascade's C++ mmap backend achieves 12.58 GB/s read,
which is 2.2$\times$ faster than GPU PCIe read (5.63 GB/s). This enables
efficient tier-migration without GPU bottleneck.
\end{tcolorbox}

