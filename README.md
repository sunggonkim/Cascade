# Cascade: HPC 스케일 LLM 추론을 위한 4계층 KV 캐시 스토리지

> **SC'26 논문** | NERSC Perlmutter | A100 GPU | Slingshot-11

---

## 🎯 핵심 결과 (실제 벤치마크)

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                        Cascade 벤치마크 요약                                  │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  📊 성능 비교 (SHM mmap vs Lustre cold read)                                  │
│  ────────────────────────────────────────────────────────────────────────    │
│  SHM mmap   ████████████████████████████████████████████ 8.61 GB/s/node     │
│  Lustre     █████ 1.09 GB/s/node                                            │
│                                                                              │
│  ⚡ Speedup: 7.9× faster                                                      │
│                                                                              │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  📈 스케일링 (4 nodes)                                                        │
│  ────────────────────────────────────────────────────────────────────────    │
│  16 ranks (4/node) ─────────────────────────── 84.3 GB/s aggregate          │
│  209.7 GB data    ─────────────────────────── 17.1 GB/s sustained           │
│                                                                              │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  💾 Deduplication (100 sessions × 1.25MB prompt)                             │
│  ────────────────────────────────────────────────────────────────────────    │
│  LMCache   ████████████████████████████████████████ 125.0 MB (100 copies)   │
│  Cascade   █ 1.25 MB (1 copy)                                                │
│                                                                              │
│  📉 Storage Saved: 100×                                                       │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

---

## 🏆 Cascade의 3가지 핵심 차별점

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    LMCache가 할 수 없는 것들                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  1. Content-Addressed Deduplication ──────────────── 100× 스토리지 절약    │
│     ┌──────────────────────────────────────────────────────────────┐       │
│     │  LMCache (세션별 ID):                                         │       │
│     │  ┌──────┐ ┌──────┐ ┌──────┐     ┌──────┐                     │       │
│     │  │Prompt│ │Prompt│ │Prompt│ ... │Prompt│  = 100개 복사본     │       │
│     │  │ #1   │ │ #2   │ │ #3   │     │ #100 │                     │       │
│     │  └──────┘ └──────┘ └──────┘     └──────┘                     │       │
│     │                                                               │       │
│     │  Cascade (SHA-256 해시):                                      │       │
│     │  ┌────────────────────────────────────────────────────────┐  │       │
│     │  │        Prompt (SHA256: a1b2c3d4...)                    │  │       │
│     │  │              1개만 저장! → 100× 절약                    │  │       │
│     │  └────────────────────────────────────────────────────────┘  │       │
│     │  Session #1 ──┐                                               │       │
│     │  Session #2 ──┼──→ 모두 같은 블록 참조                        │       │
│     │  Session #100 ┘                                               │       │
│     └──────────────────────────────────────────────────────────────┘       │
│                                                                             │
│  2. 멀티노드 SHM Aggregation ─────────────────────── 84 GB/s (4노드×4랭크) │
│     ┌──────────────────────────────────────────────────────────────┐       │
│     │  Node 1 × 4 ranks ─┐                                          │       │
│     │  Node 2 × 4 ranks ─┼─→ Aggregate: 84.3 GB/s                  │       │
│     │  Node 3 × 4 ranks ─┤   (MPI 기반 글로벌 주소 공간)             │       │
│     │  Node 4 × 4 ranks ─┘                                          │       │
│     └──────────────────────────────────────────────────────────────┘       │
│                                                                             │
│  3. 대용량 데이터 처리 ──────────────────────────── 209.7GB @ 17.1 GB/s    │
│     ┌──────────────────────────────────────────────────────────────┐       │
│     │  52.4 GB/node × 4 nodes = 209.7 GB total                      │       │
│     │  Aggregate Write: 12.46 GB/s                                  │       │
│     │  Aggregate Read:  17.06 GB/s (sustained)                      │       │
│     └──────────────────────────────────────────────────────────────┘       │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 📊 상세 벤치마크 결과

### Job 48439256: SHM vs Lustre (Core Benchmark)

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      Per-Node Performance (GB/s)                             │
├─────────────┬──────────────┬───────────────┬──────────────┬─────────────────┤
│   Storage   │    Write     │   Hot Read    │  Cold Read   │    vs Lustre    │
├─────────────┼──────────────┼───────────────┼──────────────┼─────────────────┤
│ SHM (mmap)  │    3.08      │     7.50      │    8.61*     │     7.9×        │
│ Lustre      │    0.62      │     7.96      │    1.09      │     1.0×        │
└─────────────┴──────────────┴───────────────┴──────────────┴─────────────────┘
* SHM은 항상 hot (메모리 상주)
```

### Job 48439277: Scaling Analysis

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                       Block Size Optimization                                │
├─────────────────┬────────────────┬────────────────┬─────────────────────────┤
│   Block Size    │   Write GB/s   │   Read GB/s    │   Recommendation        │
├─────────────────┼────────────────┼────────────────┼─────────────────────────┤
│     1 MB        │     2.91       │     5.51       │                         │
│    10 MB        │     3.01       │     6.26       │   ✅ Optimal            │
│    50 MB        │     2.94       │     4.03       │                         │
│   100 MB        │     2.94       │     4.16       │                         │
└─────────────────┴────────────────┴────────────────┴─────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│                       Multi-Rank Scaling                                     │
├─────────────────┬─────────────────┬──────────────────────────────────────────┤
│   Ranks/Node    │   Total Ranks   │   Aggregate Read (GB/s)                  │
├─────────────────┼─────────────────┼──────────────────────────────────────────┤
│       1         │       4         │    24.5 ████████████                     │
│       2         │       8         │    49.5 █████████████████████████        │
│       4         │      16         │    84.3 ████████████████████████████████ │
└─────────────────┴─────────────────┴──────────────────────────────────────────┘
```

### Job 48439317: Large Scale (209.7 GB)

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    Large Scale Performance                                   │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│   Total Data: 209.7 GB (52.4 GB/node × 4 nodes)                              │
│                                                                              │
│   Aggregate Write: ████████████ 12.46 GB/s                                   │
│   Aggregate Read:  █████████████████ 17.06 GB/s                              │
│                                                                              │
│   Per-node sustained read: 4.27 GB/s (over 50+ GB)                           │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 🔧 시스템 아키텍처

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         Cascade 4-Tier Architecture                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   Tier 1: GPU HBM     ─── 1555 GB/s ─── 40GB × 4 = 160GB/노드               │
│      │                     (NVIDIA A100)                                    │
│      ↓ evict (async)                                                        │
│   Tier 2: 로컬 SHM    ─── 8.6 GB/s ─── /dev/shm 256GB/노드                  │
│      │                     (mmap direct access)                             │
│      ↓ MPI RMA                                                              │
│   Tier 3: 원격 SHM    ─── 22.8 GB/s ─── Slingshot-11                        │
│      │                     (GPU-aware MPI)                                  │
│      ↓ async prefetch                                                       │
│   Tier 4: Lustre PFS  ─── 1.1 GB/s (cold) ─── $SCRATCH                      │
│                            (7.8 TB/s aggregate)                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 🚀 빠른 시작

### 벤치마크 실행

```bash
cd /pscratch/sd/s/sgkim/Skim-cascade

# SHM vs Lustre 핵심 벤치마크
sbatch benchmark/scripts/shm_vs_lustre_bench.sh

# 스케일링 벤치마크  
sbatch benchmark/scripts/scaling_bench.sh

# 대규모 벤치마크 (50GB/node)
sbatch benchmark/scripts/large_scale_bench.sh

# 결과 확인
cat benchmark/results/*_aggregate.json | jq
```

---

## 📈 벤치마크 Job IDs (재현 가능)

| Job ID | 테스트 | 결과 |
|--------|--------|------|
| 48439256 | SHM vs Lustre | ✅ 7.9× faster |
| 48439277 | Scaling | ✅ 84.3 GB/s (16 ranks) |
| 48439317 | Large Scale | ✅ 17.1 GB/s (209.7 GB) |

---

## 🔬 실험 환경 (Perlmutter)

| 구성요소 | 사양 |
|---------|------|
| **GPU** | NVIDIA A100-40GB × 4 = 160GB HBM/노드 |
| **CPU** | AMD EPYC 7763 (64 cores) |
| **DRAM** | 256GB DDR4/노드 |
| **SHM** | /dev/shm: ~135GB 사용 가능 |
| **인터커넥트** | Slingshot-11 (200 Gb/s × 4 NIC) |
| **스토리지** | Lustre $SCRATCH (44PB, 7.8 TB/s aggregate) |

---

## 🚨 연구 윤리

모든 벤치마크는 **실제 구현**을 사용합니다.

- ✅ mmap `/dev/shm` for SHM tier
- ✅ `posix_fadvise(DONTNEED)` for cold read
- ✅ SLURM Job ID로 재현 가능

**가짜 벤치마크 절대 금지:**
- ❌ 단순 Python 파일 I/O를 "LMCache", "PDC"로 레이블링 금지
- ❌ 시뮬레이션 결과를 실험 결과로 제시 금지

---

**Last Updated**: 2026-02-02  
**Author**: Sunggon Kim (sgkim@lbl.gov)
